\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{array}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{float}
\begin{document}

%==================================================
\begin{center}
    \large \textbf{Sri Sivasubramaniya Nadar College of Engineering, Chennai} \\
    (An Autonomous Institution Affiliated to Anna University) \\
    \vspace{0.3cm}
\end{center}
\begin{table}[h]
\renewcommand{\arraystretch}{1.4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Degree \& Branch & B.E. Computer Science \& Engineering & Semester & VI \\ \hline
Subject Code \& Name & \multicolumn{3}{l|}{UCS2612 – Machine Learning Algorithms Laboratory} \\ \hline
Academic Year & 2025–2026 (Even) & Batch & 2023–2027 \\ \hline
Name & Munish Madhav M & Register No. & 3122235001086 \\ \hline
Due Date & \multicolumn{3}{l|}{06.01.2026} \\ \hline
\end{tabular}}
\end{table}


\begin{center}
\textbf{Experiment 4: Binary Classification using Linear and Kernel-Based Models}
\end{center}

%==================================================
\section*{Objective}
To classify emails as spam or ham using Logistic Regression and Support Vector Machine (SVM) classifiers and to analyze the effect of hyperparameter tuning on classification performance.

%==================================================
\section*{Dataset}
The \textbf{Spambase} dataset contains numerical features extracted from email content and a binary label indicating spam or non-spam (ham).

\textbf{Dataset Links (for reference):}
\begin{itemize}
    \item Kaggle: \href{https://www.kaggle.com/datasets/somesh24/spambase}{https://www.kaggle.com/datasets/somesh24/spambase}
\end{itemize}

\section*{3. Preprocessing Steps}

The following preprocessing steps were applied to prepare the dataset for effective model training and evaluation.

\subsection*{3.1 Missing Value Check}
The dataset was examined for missing or null values. No missing values were detected, and therefore no imputation was required.

\subsection*{3.2 Feature Standardization}
All numerical features were standardized using \texttt{StandardScaler} to achieve a mean of 0 and a standard deviation of 1. Feature scaling is essential for models such as Support Vector Machines (SVM) and Logistic Regression, as these algorithms are sensitive to the relative scale of input features.

\subsection*{3.3 Train--Test Split}
The dataset was partitioned into training and testing subsets using an 80:20 split. The training set was used to learn model parameters, while the testing set was reserved for evaluating model generalization on unseen data.
\section*{4. Implementation Details}

The models were implemented using the \texttt{scikit-learn} library with the following configurations and tuning strategies.

\subsection*{4.1 Logistic Regression}
Logistic Regression models were trained using multiple solvers, including \texttt{liblinear} and \texttt{saga}. Both $L1$ (Lasso) and $L2$ (Ridge) regularization techniques were evaluated. The inverse regularization strength parameter $C$ was tuned over a predefined range to identify the optimal balance between bias and variance.

\subsection*{4.2 Support Vector Machine (SVM)}
Support Vector Machine classifiers were evaluated using four different kernel functions: Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid. Hyperparameters tuned during model selection included the regularization parameter $C$ (ranging from 0.1 to 100), the kernel coefficient \texttt{gamma} (with values \texttt{scale} and \texttt{auto}), and the polynomial degree for polynomial kernels.

\subsection*{4.3 Validation Strategy}
A 5-Fold Cross-Validation strategy was employed during hyperparameter tuning to ensure stability and robustness of the experimental results.
\section*{5. Visualizations}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{spam_class_dist.png}
\caption{class Distribution}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{spam_correlation.png}
\caption{Correlation}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{spam_hist.png}
\caption{Histogram plot}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{spam_box.png}
\caption{Boxplot}
\end{figure}
\section*{Hyperparameter Tuning Results}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|c|c|}
\hline
Model & Search Method & Best Parameters & Best CV Accuracy \\ \hline
Logistic Regression & Grid & C=10, Penalty=L1, Solver=liblinear & 0.9274 \\
SVM & Grid & C=1, Gamma=auto, Kernel=RBF & 0.9277 \\ \hline
\end{tabular}
\end{table}

%==================================================
\section*{Logistic Regression Performance}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|}
\hline
Metric & Value \\ \hline
Accuracy & 0.9153 \\
Precision & 0.9171 \\
Recall & 0.8795 \\
F1 Score & 0.8979 \\
Training Time (s) & 2.4106 \\ \hline
\end{tabular}
\end{table}

%==================================================
\section*{SVM Kernel-wise Performance}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|c|c|}
\hline
Kernel & Accuracy & F1 Score & Training Time (s) \\ \hline
Linear &  0.917481 &  0.903061   & 0.882319 \\
Polynomial & 0.764387 & 0.629060  &  2.113856 \\
RBF & 0.934853 & 0.920635  & 1.185296 \\
Sigmoid & 0.889251 &  0.866492  & 1.171060 \\ \hline
\end{tabular}
\end{table}

%==================================================
\section*{K-Fold Cross-Validation Results (K = 5)}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|c|c|c|}
\hline
Fold & Logistic Regression & SVM \\ \hline
Fold 1 &  0.919653 & 0.931596 \\
Fold 2 & 0.931522 & 0.933696 \\
Fold 3 & 0.895652 & 0.95 \\
Fold 4 &  0.95 & 0.948913 \\
Fold 5 & 0.825 & 0.847826  \\ \hline
Average & 0.904365 & 0.922406 \\ \hline
\end{tabular}
\end{table}

%==================================================
\section*{Comparative Analysis}

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|c|}
\hline
Criterion & Logistic Regression & SVM \\ \hline
Accuracy & 91.53\% & 93.49\% \\ \hline
Model Complexity & Low & High \\ \hline
Training Time & Low & High \\ \hline
Interpretability & High & Low \\ \hline
\end{tabular}
\end{table}
\section*{Performance Analysis and Discussion}

The Support Vector Machine (SVM) with the Radial Basis Function (RBF) kernel emerged as the best-performing classifier in this experiment. It achieved the highest test accuracy of 93.49\% and an F1 score of 0.9206, outperforming the tuned Logistic Regression model, which recorded an accuracy of 91.53\% and an F1 score of 0.8979. This indicates that the margin-based optimization of SVM was more effective than the probabilistic decision boundary of Logistic Regression for this dataset.

Regularization played a crucial role in improving model performance. For Logistic Regression, the optimal configuration selected through grid search employed L1 regularization with an inverse regularization strength of $C = 10$. The choice of L1 regularization enabled implicit feature selection by driving the coefficients of less informative features to zero, thereby reducing noise. The relatively high value of $C$ indicates weak regularization, suggesting that a closer fit to the training data was necessary to capture meaningful patterns in the spam classification task.

The choice of kernel significantly influenced the performance of the SVM models. The RBF kernel achieved the best results, confirming the presence of complex and non-linear decision boundaries between spam and non-spam emails. The linear kernel also performed competitively with an accuracy of 91.75\%, indicating that the data is largely linearly separable. However, the RBF kernel was able to capture subtle non-linear variations that the linear kernel could not. In contrast, the polynomial kernel performed poorly with an accuracy of 76.44\%, indicating underfitting and an ineffective feature mapping for this dataset.

An analysis of the bias--variance trade-off further explains these results. Logistic Regression, being a linear model, exhibited higher bias, which limited its ability to model non-linear relationships in the data. This resulted in comparatively lower predictive performance. The SVM with the RBF kernel achieved a more optimal balance between bias and variance. Its high accuracy reflects low bias, while the close agreement between cross-validation accuracy (92.77\%) and test accuracy (93.49\%) indicates low variance and good generalization. The selected regularization parameter $C = 1$ provided sufficient flexibility to model complex patterns without overfitting.



%==================================================
\section*{Learning Outcomes}
\begin{itemize}
    \item Understand probabilistic and margin-based classifiers.
    \item Apply hyperparameter tuning.
    \item Evaluate classification models.
    \item Interpret experimental results.
\end{itemize}

%==================================================
\section*{References}
\begin{itemize}
    \item \href{https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression}{Scikit-learn: Logistic Regression}
    \item \href{https://scikit-learn.org/stable/modules/svm.html}{Scikit-learn: Support Vector Machines}
    \item \href{https://scikit-learn.org/stable/modules/grid_search.html}{Scikit-learn: Hyperparameter Optimization}
    \item \href{https://www.kaggle.com/datasets/somesh24/spambase}{Spambase Dataset – Kaggle}
    \item \href{https://archive.ics.uci.edu/ml/datasets/spambase}{UCI ML Repository – Spambase}
\end{itemize}

\end{document}
