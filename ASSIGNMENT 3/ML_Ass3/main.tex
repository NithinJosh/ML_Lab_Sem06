\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{array}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{titlesec}
\usepackage{float}

\begin{document}

%==================================================
\begin{center}
\textbf{\Large Sri Sivasubramaniya Nadar College of Engineering, Chennai}\\
\textbf{(An Autonomous Institution affiliated to Anna University)}
\end{center}

\begin{table}[h]
\renewcommand{\arraystretch}{1.4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Degree \& Branch & B.E. Computer Science \& Engineering & Semester & VI \\ \hline
Subject Code \& Name & \multicolumn{3}{l|}{UCS2612 – Machine Learning Algorithms Laboratory} \\ \hline
Academic Year & 2025–2026 (Even) & Batch & 2023–2027 \\ \hline
Name & Munish Madhav M & Register No. & 3122235001086 \\ \hline
Due Date & \multicolumn{3}{l|}{06.01.2026} \\ \hline
\end{tabular}}
\end{table}

\begin{center}
\textbf{ Experiment 3:}
\textbf{Regression Analysis using Linear and Regularized Model}
\end{center}

%==================================================
\section*{1. Aim and Objective}
To implement linear and regularized regression models for predicting a continuous target variable, evaluate their performance using multiple metrics, visualize model behavior, and analyze overfitting, underfitting, and bias--variance characteristics.

%==================================================
\section*{2. Dataset}
A real-world regression dataset containing numerical and categorical features related to loan applications is used.  
The target variable is the \textbf{loan amount sanctioned}.

\section*{3. Preprocessing Steps}

A systematic preprocessing pipeline was implemented using \texttt{sklearn.pipeline} to ensure consistent and reproducible data transformations across training and testing datasets.

\subsection*{Handling Missing Values}

\subsubsection*{Numerical Data}
Missing values in numerical features were imputed using the \textit{median} strategy. This approach is robust to outliers and helps maintain the integrity of feature distributions.

\subsubsection*{Categorical Data}
Missing values in categorical features were imputed using the \textit{most\_frequent} (mode) strategy, preserving the most commonly occurring category in each feature.

\subsection*{Encoding Categorical Variables}

Categorical features were encoded using \textbf{One-Hot Encoding} via \texttt{OneHotEncoder}. This technique converts categorical variables into binary indicator vectors. The parameter \texttt{handle\_unknown = ignore} was used to safely handle unseen categories in the test dataset.

\subsection*{Feature Scaling}

Numerical features were standardized using \textbf{Standard Scaling} with \texttt{StandardScaler}. This transformation scales features to have zero mean and unit variance. Feature scaling is particularly important for regularized linear models such as Ridge and Lasso, as it ensures that regularization penalties are applied uniformly across all features.

\section*{4. Implementation Details}

The experiment involved a comparative analysis of four regression models to evaluate their predictive performance and robustness.

\subsection*{Regression Models}

\subsubsection*{Linear Regression (Baseline)}
Linear Regression was implemented as the baseline model using standard Ordinary Least Squares (OLS) optimization.

\subsubsection*{Ridge Regression (L2 Regularization)}
Ridge Regression introduces an $L2$ regularization term that penalizes the square of the magnitude of coefficients, helping to reduce model variance.

\textbf{Hyperparameter Grid:}
\[
\alpha \in \{0.01, 0.1, 1, 10, 100\}
\]

\subsubsection*{Lasso Regression (L1 Regularization)}
Lasso Regression applies an $L1$ penalty, which encourages sparsity by driving less important feature coefficients to zero.

\textbf{Hyperparameter Grid:}
\[
\alpha \in \{0.001, 0.01, 0.1, 1, 10\}
\]

\subsubsection*{Elastic Net Regression}
Elastic Net Regression combines both $L1$ and $L2$ regularization, balancing sparsity and coefficient shrinkage.

\textbf{Hyperparameter Grid:}
\[
\alpha \in \{0.01, 0.1, 1, 10\}, \quad
l1\_ratio \in \{0.2, 0.5, 0.8\}
\]

\subsection*{Validation Strategy}

\subsubsection*{K-Fold Cross-Validation}
A $K$-Fold Cross-Validation strategy with $K=5$ folds was employed during grid search to ensure robust and unbiased hyperparameter selection.

\subsubsection*{Scoring Metric}
The coefficient of determination ($R^2$ score) was used as the primary evaluation metric for model optimization and comparison.
\section*{5. Visualizations}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{target_dist_loan.png}
\caption{Target Distribution}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{loan_correlation_final.png}
\caption{Correlation}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{coefficient_comparison.png}
\caption{Coefficient comparison bar plot}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{training_vs_validation.png}
\caption{Training error vs. validation error plot}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{loan_scatter.png}
\caption{Scatter plot ofvarious features}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{predicted_residual.png}
\caption{predicted vs acutal and residual plots}
\end{figure}
\section*{6. Performance Tables}
\subsection*{Hyperparameter Tuning Results}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Hyperparameter Tuning Summary}
\begin{tabular}{|l|c|c|c|}
\hline
Model & Search Method & Best Parameters & Best CV $R^2$ \\ \hline
Ridge Regression & Grid & alpha:100 &  0.5786\\
Lasso Regression & Grid & alpha:10 & 0.5785 \\
Elastic Net Regression & Grid & alpha: 0.1, l1_ratio: 0.5 & 0.5798 \\ \hline
\end{tabular}
\end{table}
\subsection*{Cross-Validation Performance (K = 5)}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Cross-Validation Performance}
\begin{tabular}{|l|c|c|c|c|}
\hline
Model & MAE & MSE & RMSE & $R^2$ \\ \hline
Linear Regression &  21508.1142 & 984727093.1145  & 31380.3616  & 0.5790 \\
Ridge Regression & 21524.7473  & 983019851.6404   & 31353.1474 & 0.5797  \\
Lasso Regression &  21497.8842 & 983433813.8426 & 31359.7483  & 0.5796 \\
Elastic Net Regression & 21732.7267  & 980735482.7595 & 31316.6965 & 0.5807 \\ \hline
\end{tabular}
\end{table}
\subsection*{Test Set Performance Comparison}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Test Set Performance}
\begin{tabular}{|l|c|c|c|c|}
\hline
Model & MAE & MSE & RMSE & $R^2$ \\ \hline
Linear Regression &  21589.8579 & 1.018912e+09  & 31920.3984  & 0.5511 \\
Ridge Regression & 21582.3204   & 1.017227e+09  & 31893.9988 & 0.5518 \\
Lasso Regression & 21564.5711 & 1.017745e+09 & 31902.1118  & 0.5516  \\
Elastic Net Regression & 21751.6900  & 1.018489e+09  & 31913.7778  & 0.5513  \\ \hline
\end{tabular}
\end{table}
\subsection*{Effect of Regularization on Coefficients}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\caption{Coefficient Comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
Feature & Linear & Ridge & Lasso & Elastic Net \\ \hline
Feature 1 & 47,656.54 & 1,039.48 & 3.85 & 97.46 \\
Feature 2 & 35,365.37 & 33,825.79 & 35,169.96 & 25,155.47  \\
Feature 3 & -47,644.12 & -1,031.74 & 0.00 & -86.51 \\ \hline
\end{tabular}
\end{table}
\section*{7. Overfitting and Underfitting Analysis}

\subsection*{Training vs. Validation Gap}
The cross-validation $R^2$ scores (approximately 0.579) are consistently higher than the test set $R^2$ scores (approximately 0.551). Although a reduction in performance is observed on unseen data, the gap remains relatively small ($< 0.03$), indicating that the model does not suffer from significant overfitting.

\subsection*{Underfitting Analysis}
Despite stable generalization performance, the overall $R^2$ score of around 0.55 suggests that the model explains only 55\% of the variance in the target variable. This indicates underfitting. The linear assumption inherent in the models may be overly simplistic for capturing complex relationships present in financial data, such as non-linear interactions between property value and loan approval amounts.

\subsection*{Effect of Regularization}
The difference in test $R^2$ scores between the baseline Linear Regression model (0.5511) and the best-performing regularized model (Ridge Regression: 0.5518) is negligible. This observation confirms that high variance or overfitting was not the dominant issue. Instead, model bias and limited representational capacity were the primary factors constraining performance.
\section*{8. Bias--Variance Analysis}

\subsection*{Bias Behavior}
The Linear Regression model exhibits high bias, as it consistently fails to capture the full complexity of the underlying data structure. This behavior is reflected in a relatively high Mean Absolute Error (MAE) of approximately 21{,}500, indicating systematic prediction errors.

\subsection*{Variance Reduction}
\textbf{Linear Regression:} The baseline Linear Regression model demonstrated high variance in its learned coefficients. Certain feature weights were excessively large (e.g., Feature 2 with a coefficient of approximately 47{,}656), making the model sensitive to minor variations in input data and leading to unstable predictions.

\textbf{Ridge and Elastic Net Regression:} Regularized models effectively mitigated coefficient variance. Ridge Regression significantly shrunk the Feature 2 coefficient from 47{,}656 to approximately 1{,}039, improving model robustness and generalization, even though the improvement in predictive accuracy was modest.

\subsection*{Feature Sparsity (Lasso Regression)}
Lasso Regression demonstrated effective feature selection by driving the coefficients of several redundant or less informative features (e.g., Features 13, 17, and 18) to exactly zero. This sparsity reduces overall model complexity and indicates that not all collected applicant attributes contribute meaningfully to predicting the loan sanction amount.
\section*{9. Observations and Conclusion}

\subsection*{Feature Importance}
The analysis revealed that \textit{Loan Amount Request (USD)} and \textit{Credit Score} were the most significant predictors of the loan sanction amount. Correlation analysis correctly identified the Loan Amount Request as having the strongest positive relationship with the target variable.

\subsection*{Model Performance}
All four regression models demonstrated comparable performance, with Ridge Regression marginally outperforming the others, achieving an $R^2$ score of 0.5518. The close similarity in performance across models suggests that the dataset has an inherent limitation on how effectively it can be modeled using linear relationships.

\subsection*{Conclusion}
Regularization techniques proved effective in stabilizing model parameters by mitigating coefficient explosion and enabling feature selection in the case of Lasso Regression. However, these techniques were unable to overcome the fundamental underfitting associated with linear models. To achieve higher predictive accuracy (greater than $0.60$ in $R^2$), future work should explore non-linear modeling approaches such as Random Forests or Gradient Boosting methods.


\section*{Dataset reference}:
\begin{itemize}
    \item Kaggle: \href{https://www.kaggle.com/datasets/phileinsophos/predict-loan-amount-data}{Predict Loan Amount Data}
\end{itemize}

\section*{References}
\begin{itemize}
    \item \href{https://scikit-learn.org/stable/modules/linear_model.html}{Scikit-learn: Linear Models}
    \item \href{https://scikit-learn.org/stable/modules/grid_search.html}{Scikit-learn: Hyperparameter Optimization}
    \item \href{https://www.kaggle.com/datasets/phileinsophos/predict-loan-amount-data}{Loan Amount Dataset}
\end{itemize}

\end{document}
